{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Uds5r4QWcB5bsELiyBM8TBHEHFVCVcd3","timestamp":1726019525218}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RQi9cWiKCttl"},"source":["# **Deep Learning on 3D Meshes**"]},{"cell_type":"markdown","metadata":{"id":"GFwZT37OVZ9y"},"source":["# Installation\n","Installing the required packages"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XKMQHgqd7bx","executionInfo":{"status":"ok","timestamp":1726022397687,"user_tz":360,"elapsed":604722,"user":{"displayName":"Karl Strickland","userId":"11999938059888559217"}},"outputId":"43e07edc-bfad-405f-8e72-1300b81821fc"},"source":["!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install torch-geometric\n","!pip install trimesh"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","Collecting torch-scatter\n","  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: torch-scatter\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3658839 sha256=95f1ddb08a14d5fddd4d81b88a5b51d9b68d561ca27bf914a0ee849344627a01\n","  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n","Successfully built torch-scatter\n","Installing collected packages: torch-scatter\n","Successfully installed torch-scatter-2.1.2\n","Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","Collecting torch-sparse\n","  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n","Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n","Building wheels for collected packages: torch-sparse\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl size=2809240 sha256=7db1698e0b05db73586883f21ad79860351cee15b485963194dbf021184c6582\n","  Stored in directory: /root/.cache/pip/wheels/c9/dd/0f/a6a16f9f3b0236733d257b4b4ea91b548b984a341ed3b8f38c\n","Successfully built torch-sparse\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.18\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.3.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n","Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-geometric\n","Successfully installed torch-geometric-2.5.3\n","Collecting trimesh\n","  Downloading trimesh-4.4.9-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.26.4)\n","Downloading trimesh-4.4.9-py3-none-any.whl (700 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.1/700.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trimesh\n","Successfully installed trimesh-4.4.9\n"]}]},{"cell_type":"code","metadata":{"id":"Fyqne3fekGyG"},"source":["from time import sleep\n","from pathlib import Path\n","from itertools import tee\n","from functools import lru_cache\n","\n","import trimesh\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch_geometric.nn import MessagePassing\n","from torch_geometric.utils import add_self_loops, remove_self_loops\n","from torch_geometric.transforms import BaseTransform, Compose, FaceToEdge\n","from torch_geometric.data import Data, InMemoryDataset, extract_zip, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jjWe7FMVlhu"},"source":["#Download Annotated MPI FAUST Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJFaCYLng_TK","executionInfo":{"status":"ok","timestamp":1638920358587,"user_tz":480,"elapsed":5369,"user":{"displayName":"Paul Aurel Diederichs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15477176261249902325"}},"outputId":"d6b3f4b9-106b-4343-8e45-909f4773704b"},"source":["!gdown --id 1CvfkR6iFOpfo0yRyaVOvgv2piphP6pze"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1CvfkR6iFOpfo0yRyaVOvgv2piphP6pze\n","To: /content/MPI-FAUST.zip\n","100% 14.6M/14.6M [00:00<00:00, 46.3MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"3tRDP4G_VPgw"},"source":["##Device\n","\n","You might need to use GPU for this Colab.\n","Please click Runtime and then Change runtime type. Then set the hardware accelerator to GPU."]},{"cell_type":"code","metadata":{"id":"2JcrrNbKBHyj"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gr6HXz0qqSxd"},"source":["def load_mesh(mesh_filename: Path):\n","    \"\"\"Extract vertices and faces from raw mesh file.\n","\n","    Parameters\n","    ----------\n","    mesh_filename: PathLike\n","        Path to mesh `.ply` file.\n","\n","    Returns\n","    -------\n","    vertices: torch.tensor\n","        Float tensor of size (|V|, 3), where each row\n","        specifies the spatial position of a vertex in 3D space.\n","    faces: torch.tensor\n","        Intger tensor of size (|M|, 3), where each row\n","        defines a traingular face.\n","    \"\"\"\n","    mesh = trimesh.load_mesh(mesh_filename, process=False)\n","    vertices = torch.from_numpy(mesh.vertices).to(torch.float)\n","    faces = torch.from_numpy(mesh.faces)\n","    faces = faces.t().to(torch.long).contiguous()\n","    return vertices, faces"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmQFG-sEOmGc"},"source":["# Dataset Defintion"]},{"cell_type":"code","metadata":{"id":"HvgKq75uiIar"},"source":["#Dataset implementation for our faust data (This may already be done by the Faust dataset loader that pytorch implemented)\n","class SegmentationFaust(InMemoryDataset):\n","    #Our label map.  Not sure that the faust dataset loader has this\n","    map_seg_label_to_id = dict(\n","        head=0,\n","        torso=1,\n","        left_arm=2,\n","        left_hand=3,\n","        right_arm=4,\n","        right_hand=5,\n","        left_upper_leg=6,\n","        left_lower_leg=7,\n","        left_foot=8,\n","        right_upper_leg=9,\n","        right_lower_leg=10,\n","        right_foot=11,\n","    )\n","\n","    def __init__(self, root, train: bool = True, pre_transform=None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        root: PathLike\n","            Root directory where the dataset should be saved.\n","        train: bool\n","            Whether to load training data or test data.\n","        pre_transform: Optional[Callable]\n","            A function that takes in a torch_geometric.data.Data object\n","            and outputs a transformed version. Note that the transformed\n","            data object will be saved to disk.\n","\n","        \"\"\"\n","        super().__init__(root, pre_transform)\n","        path = self.processed_paths[0] if train else self.processed_paths[1]\n","        self.data, self.slices = torch.load(path)\n","\n","    @property\n","    def processed_file_names(self) -> list:\n","        return [\"training.pt\", \"test.pt\"]\n","\n","    @property\n","    @lru_cache(maxsize=32)\n","    def _segmentation_labels(self):\n","        \"\"\"Extract segmentation labels.\"\"\"\n","        path_to_labels = Path(self.root) / \"MPI-FAUST\"/ \"segmentations.npz\"\n","        seg_labels = np.load(str(path_to_labels))[\"segmentation_labels\"]\n","        return torch.from_numpy(seg_labels).type(torch.int64)\n","\n","    def _mesh_filenames(self):\n","        \"\"\"Extract all mesh filenames.\"\"\"\n","        path_to_meshes = Path(self.root)/ \"MPI-FAUST\" / \"meshes\"\n","        return path_to_meshes.glob(\"*.ply\")\n","\n","    def _unzip_dataset(self):\n","        \"\"\"Extract dataset from zip.\"\"\"\n","        path_to_zip = Path(self.root) / \"MPI-FAUST.zip\"\n","        extract_zip(str(path_to_zip), self.root, log=False)\n","\n","    def process(self):\n","        \"\"\"Process the raw meshes files and their corresponding class labels.\"\"\"\n","        self._unzip_dataset()\n","\n","        data_list = []\n","        for mesh_filename in sorted(self._mesh_filenames()):\n","            vertices, faces = load_mesh(mesh_filename)\n","            data = Data(x=vertices, face=faces)\n","            data.segmentation_labels = self._segmentation_labels\n","            if self.pre_transform is not None:\n","                data = self.pre_transform(data)\n","            data_list.append(data)\n","\n","        torch.save(self.collate(data_list[:80]), self.processed_paths[0])\n","        torch.save(self.collate(data_list[80:]), self.processed_paths[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzKPu4DEkTuG"},"source":["class NormalizeUnitSphere(BaseTransform):\n","    \"\"\"Center and normalize node-level features to unit length.\"\"\"\n","\n","    @staticmethod\n","    def _re_center(x):\n","        \"\"\"Recenter node-level features onto feature centroid.\"\"\"\n","        centroid = torch.mean(x, dim=0)\n","        return x - centroid\n","\n","    @staticmethod\n","    def _re_scale_to_unit_length(x):\n","        \"\"\"Rescale node-level features to unit-length.\"\"\"\n","        max_dist = torch.max(torch.norm(x, dim=1))\n","        return x / max_dist\n","\n","    def __call__(self, data: Data):\n","        if data.x is not None:\n","            data.x = self._re_scale_to_unit_length(self._re_center(data.x))\n","\n","        return data\n","\n","    def __repr__(self):\n","        return \"{}()\".format(self.__class__.__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZihIbZD2O4D"},"source":["#Network Definitions\n","\n","We define the mesh segmentation network. It consists of"]},{"cell_type":"markdown","metadata":{"id":"iRWj56_z3OyY"},"source":["##Helper Functions\n","We define helper functions to simplify the creation of graph convolutional layers and multilayered perceptrons."]},{"cell_type":"code","metadata":{"id":"FsbC8UyJspov"},"source":["def pairwise(iterable):\n","    \"\"\"Iterate over all pairs of consecutive items in a list.\n","    Notes\n","    -----\n","        [s0, s1, s2, s3, ...] -> (s0,s1), (s1,s2), (s2, s3), ...\n","    \"\"\"\n","    a, b = tee(iterable)\n","    next(b, None)\n","    return zip(a, b)\n","\n","def get_conv_layers(channels: list, conv: MessagePassing, conv_params: dict):\n","    \"\"\"Define convolution layers with specified in and out channels.\n","\n","    Parameters\n","    ----------\n","    channels: list\n","        List of integers specifying the size of the convolution channels.\n","    conv: MessagePassing\n","        Convolution layer.\n","    conv_params: dict\n","        Dictionary specifying convolution parameters.\n","\n","    Returns\n","    -------\n","    list\n","        List of convolutions with the specified channels.\n","    \"\"\"\n","    conv_layers = [\n","        conv(in_ch, out_ch, **conv_params) for in_ch, out_ch in pairwise(channels)\n","    ]\n","    return conv_layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vV4G432eAJ2s"},"source":["def get_mlp_layers(channels: list, activation, output_activation=nn.Identity):\n","    \"\"\"Define basic multilayered perceptron network.\"\"\"\n","    layers = []\n","    *intermediate_layer_definitions, final_layer_definition = pairwise(channels)\n","\n","    for in_ch, out_ch in intermediate_layer_definitions:\n","        intermediate_layer = nn.Linear(in_ch, out_ch)\n","        layers += [intermediate_layer, activation()]\n","\n","    layers += [nn.Linear(*final_layer_definition), output_activation()]\n","    return nn.Sequential(*layers)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzPBziZ82uJo"},"source":["##Feature-Steered Graph Convolution\n","\n","The foundation for our graph neural network are feature-steered graph convolution ([Verma](https://arxiv.org/pdf/1706.05206.pdf) et al. (2018)). They perform the following layer-wise node-level feature update:\n","\n","$$ \\mathbf{h}_v^{(l)} = \\mathbf{b} + \\sum_{m=1}^{M}  \\frac{1}{|\\mathcal{N}_{v}|} \\sum_{u \\in \\mathcal{N}_v} \\alpha_m^{(l)}(v, u)\\mathbf{W}^{(l)}_{m} \\mathbf{h}^{(l-1)}_{u}$$\n","\n","where $\\alpha_m^{(l)}(v, u)$ is the learned attention weight of the $m$-th attention mechanism, $\\mathbf{W}_m^{(l)}$ is the associated linear transformation matrix, $\\mathcal{N}_v$\n","is the set of adjacent vertices of vertex $v$ (including $v$ itself), and $|\\mathcal{N}_v|$ the set's cardinal.\n","\n","The attention coefficients for each layer $l$ are computed as follows:\n","$$\\alpha_m^{(l)}(v, u) = \\frac{\\exp(\\mathbf{u}^{(l) \\ T}_m(\\mathbf{h}^{(l-1)}_u - \\mathbf{h}^{(l-1)}_v) + c_m^{(l)})}{\\sum_{m=1}^{M} \\exp(\\mathbf{u}^{(l) \\ T}_m(\\mathbf{h}^{(l-1)}_u - \\mathbf{h}^{(l-1)}_v) + c_m^{(l)})}, \\quad \\quad \\sum_{m=1}^{M}\\alpha_m^{(l)}(i,j) = 1$$\n","\n","where $\\mathbf{u}_m^{(l)}$ and $c_m^{(l)}$ are learnable parameters, specific to each layer $l$. The attention coefficients are normalised so that they sum to $1$.\n","\n","\n","We implement this graph convolution using [**PyG's MessagePassing**](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html) class."]},{"cell_type":"code","metadata":{"id":"s_vWPdepsu6b"},"source":["class FeatureSteeredConvolution(MessagePassing):\n","    \"\"\"Implementation of feature steered convolutions.\n","\n","    References\n","    ----------\n","    .. [1] Verma, Nitika, Edmond Boyer, and Jakob Verbeek.\n","       \"Feastnet: Feature-steered graph convolutions for 3d shape analysis.\"\n","       Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        num_heads: int,\n","        ensure_trans_invar: bool = True,\n","        bias: bool = True,\n","        with_self_loops: bool = True,\n","    ):\n","        super().__init__(aggr=\"mean\")\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.num_heads = num_heads\n","        self.with_self_loops = with_self_loops\n","\n","        self.linear = torch.nn.Linear(\n","            in_features=in_channels,\n","            out_features=out_channels * num_heads,\n","            bias=False,\n","        )\n","        self.u = torch.nn.Linear(\n","            in_features=in_channels,\n","            out_features=num_heads,\n","            bias=False,\n","        )\n","        self.c = torch.nn.Parameter(torch.Tensor(num_heads))\n","\n","        if not ensure_trans_invar:\n","            self.v = torch.nn.Linear(\n","                in_features=in_channels,\n","                out_features=num_heads,\n","                bias=False,\n","            )\n","        else:\n","            self.register_parameter(\"v\", None)\n","\n","        if bias:\n","            self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter(\"bias\", None)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        \"\"\"Initialization of tuneable network parameters.\"\"\"\n","        torch.nn.init.uniform_(self.linear.weight)\n","        torch.nn.init.uniform_(self.u.weight)\n","        torch.nn.init.normal_(self.c, mean=0.0, std=0.1)\n","        if self.bias is not None:\n","            torch.nn.init.normal_(self.bias, mean=0.0, std=0.1)\n","        if self.v is not None:\n","            torch.nn.init.uniform_(self.v.weight)\n","\n","    def forward(self, x, edge_index):\n","        \"\"\"Forward pass through a feature steered convolution layer.\n","\n","        Parameters\n","        ----------\n","        x: torch.tensor [|V|, in_features]\n","            Input feature matrix, where each row describes\n","            the input feature descriptor of a node in the graph.\n","        edge_index: torch.tensor [2, E]\n","            Edge matrix capturing the graph's\n","            edge structure, where each row describes an edge\n","            between two nodes in the graph.\n","        Returns\n","        -------\n","        torch.tensor [|V|, out_features]\n","            Output feature matrix, where each row corresponds\n","            to the updated feature descriptor of a node in the graph.\n","        \"\"\"\n","        if self.with_self_loops:\n","            edge_index, _ = remove_self_loops(edge_index)\n","            edge_index, _ = add_self_loops(edge_index=edge_index, num_nodes=x.shape[0])\n","\n","        out = self.propagate(edge_index, x=x)\n","        return out if self.bias is None else out + self.bias\n","\n","    def _compute_attention_weights(self, x_i, x_j):\n","        \"\"\"Computation of attention weights.\n","\n","        Parameters\n","        ----------\n","        x_i: torch.tensor [|E|, in_feature]\n","            Matrix of feature embeddings for all central nodes,\n","            collecting neighboring information to update its embedding.\n","        x_j: torch.tensor [|E|, in_features]\n","            Matrix of feature embeddings for all neighboring nodes\n","            passing their messages to the central node along\n","            their respective edge.\n","        Returns\n","        -------\n","        torch.tensor [|E|, M]\n","            Matrix of attention scores, where each row captures\n","            the attention weights of transformed node in the graph.\n","        \"\"\"\n","        if x_j.shape[-1] != self.in_channels:\n","            raise ValueError(\n","                f\"Expected input features with {self.in_channels} channels.\"\n","                f\" Instead received features with {x_j.shape[-1]} channels.\"\n","            )\n","        if self.v is None:\n","            attention_logits = self.u(x_i - x_j) + self.c\n","        else:\n","            attention_logits = self.u(x_i) + self.b(x_j) + self.c\n","        return F.softmax(attention_logits, dim=1)\n","\n","    def message(self, x_i, x_j):\n","        \"\"\"Message computation for all nodes in the graph.\n","\n","        Parameters\n","        ----------\n","        x_i: torch.tensor [|E|, in_feature]\n","            Matrix of feature embeddings for all central nodes,\n","            collecting neighboring information to update its embedding.\n","        x_j: torch.tensor [|E|, in_features]\n","            Matrix of feature embeddings for all neighboring nodes\n","            passing their messages to the central node along\n","            their respective edge.\n","        Returns\n","        -------\n","        torch.tensor [|E|, out_features]\n","            Matrix of updated feature embeddings for\n","            all nodes in the graph.\n","        \"\"\"\n","        attention_weights = self._compute_attention_weights(x_i, x_j)\n","        x_j = self.linear(x_j).view(-1, self.num_heads, self.out_channels)\n","        return (attention_weights.view(-1, self.num_heads, 1) * x_j).sum(dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8-XGHac3Hp3"},"source":["##Graph Neural Network\n","We create a graph neural network to derive a node-level feature embedding by stacking featured steered graph convolutions sequentially."]},{"cell_type":"code","metadata":{"id":"IKVkmSbjk5dF"},"source":["class GraphFeatureEncoder(torch.nn.Module):\n","    \"\"\"Graph neural network consisting of stacked graph convolutions.\"\"\"\n","    def __init__(\n","        self,\n","        in_features,\n","        conv_channels,\n","        num_heads,\n","        apply_batch_norm: int = True,\n","        ensure_trans_invar: bool = True,\n","        bias: bool = True,\n","        with_self_loops: bool = True,\n","    ):\n","        super().__init__()\n","\n","        conv_params = dict(\n","            num_heads=num_heads,\n","            ensure_trans_invar=ensure_trans_invar,\n","            bias=bias,\n","            with_self_loops=with_self_loops,\n","        )\n","        self.apply_batch_norm = apply_batch_norm\n","\n","        *first_conv_channels, final_conv_channel = conv_channels\n","        conv_layers = get_conv_layers(\n","            channels=[in_features] + conv_channels,\n","            conv=FeatureSteeredConvolution,\n","            conv_params=conv_params,\n","        )\n","        self.conv_layers = nn.ModuleList(conv_layers)\n","\n","        self.batch_layers = [None for _ in first_conv_channels]\n","        if apply_batch_norm:\n","            self.batch_layers = nn.ModuleList(\n","                [nn.BatchNorm1d(channel) for channel in first_conv_channels]\n","            )\n","\n","    def forward(self, x, edge_index):\n","        *first_conv_layers, final_conv_layer = self.conv_layers\n","        for conv_layer, batch_layer in zip(first_conv_layers, self.batch_layers):\n","            x = conv_layer(x, edge_index)\n","            x = F.relu(x)\n","            if batch_layer is not None:\n","                x = batch_layer(x)\n","        return final_conv_layer(x, edge_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrQHPI_i3BYj"},"source":["##General Network Definition\n","The network architecture consists of three components: a MLP encoder network, a graph neural network, a MLP class prediction head."]},{"cell_type":"code","metadata":{"id":"9_2-KscKr4c8"},"source":["class MeshSeg(torch.nn.Module):\n","    \"\"\"Mesh segmentation network.\"\"\"\n","    def __init__(\n","        self,\n","        in_features,\n","        encoder_features,\n","        conv_channels,\n","        encoder_channels,\n","        decoder_channels,\n","        num_classes,\n","        num_heads,\n","        apply_batch_norm=True,\n","    ):\n","        super().__init__()\n","        self.input_encoder = get_mlp_layers(\n","            channels=[in_features] + encoder_channels,\n","            activation=nn.ReLU,\n","        )\n","        self.gnn = GraphFeatureEncoder(\n","            in_features=encoder_features,\n","            conv_channels=conv_channels,\n","            num_heads=num_heads,\n","            apply_batch_norm=apply_batch_norm,\n","        )\n","        *_, final_conv_channel = conv_channels\n","\n","        self.final_projection = get_mlp_layers(\n","            [final_conv_channel] + decoder_channels + [num_classes],\n","            activation=nn.ReLU,\n","        )\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.input_encoder(x)\n","        x = self.gnn(x, edge_index)\n","        return self.final_projection(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2tKpiUM5OtmU"},"source":["#Training Pipeline\n","\n","We define the training and evaluation pipeline."]},{"cell_type":"code","metadata":{"id":"FYwygTl31zJR"},"source":["def train(net, train_data, optimizer, loss_fn, device):\n","    \"\"\"Train network on training dataset.\"\"\"\n","    net.train()\n","    cumulative_loss = 0.0\n","    for data in train_data:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out = net(data)\n","        loss = loss_fn(out, data.segmentation_labels.squeeze())\n","        loss.backward()\n","        cumulative_loss += loss.item()\n","        optimizer.step()\n","    return cumulative_loss / len(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c35Tz1ONtBNa"},"source":["def accuracy(predictions, gt_seg_labels):\n","    \"\"\"Compute accuracy of predicted segmentation labels.\n","\n","    Parameters\n","    ----------\n","    predictions: [|V|, num_classes]\n","        Soft predictions of segmentation labels.\n","    gt_seg_labels: [|V|]\n","        Ground truth segmentations labels.\n","    Returns\n","    -------\n","    float\n","        Accuracy of predicted segmentation labels.\n","    \"\"\"\n","    predicted_seg_labels = predictions.argmax(dim=-1, keepdim=True)\n","    if predicted_seg_labels.shape != gt_seg_labels.shape:\n","        raise ValueError(\"Expected Shapes to be equivalent\")\n","    correct_assignments = (predicted_seg_labels == gt_seg_labels).sum()\n","    num_assignemnts = predicted_seg_labels.shape[0]\n","    return float(correct_assignments / num_assignemnts)\n","\n","\n","def evaluate_performance(dataset, net, device):\n","    \"\"\"Evaluate network performance on given dataset.\n","\n","    Parameters\n","    ----------\n","    dataset: DataLoader\n","        Dataset on which the network is evaluated on.\n","    net: torch.nn.Module\n","        Trained network.\n","    device: str\n","        Device on which the network is located.\n","\n","    Returns\n","    -------\n","    float:\n","        Mean accuracy of the network's prediction on\n","        the provided dataset.\n","    \"\"\"\n","    prediction_accuracies = []\n","    for data in dataset:\n","        data = data.to(device)\n","        predictions = net(data)\n","        prediction_accuracies.append(accuracy(predictions, data.segmentation_labels))\n","    return sum(prediction_accuracies) / len(prediction_accuracies)\n","\n","@torch.no_grad()\n","def test(net, train_data, test_data, device):\n","    net.eval()\n","    train_acc = evaluate_performance(train_data, net, device)\n","    test_acc = evaluate_performance(test_data, net, device)\n","    return train_acc, test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIhXX4icw-3g"},"source":["#Dataset and Preprocessing\n","Next, we instantiate the mesh segmentation network. Furthermore, we load both the training and test datasets. Note that we normalize all meshes to unit length and convert the mesh's face matrix into an adjacency matrix by applying pre-transformations."]},{"cell_type":"code","metadata":{"id":"5WQtyzbaBPej"},"source":["model_params = dict(\n","    in_features=3,\n","    encoder_features=16,\n","    conv_channels=[32, 64, 128, 64],\n","    encoder_channels=[16],\n","    decoder_channels=[32],\n","    num_classes=12,\n","    num_heads=12,\n","    apply_batch_norm=True,\n",")\n","\n","net = MeshSeg(**model_params).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZtv9rj7tLRC","executionInfo":{"status":"ok","timestamp":1638920367335,"user_tz":480,"elapsed":379,"user":{"displayName":"Paul Aurel Diederichs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15477176261249902325"}},"outputId":"573a1b6f-7e11-4f45-a22b-142c96ec0b92"},"source":["root = \"/content\"\n","pre_transform = Compose([FaceToEdge(remove_faces=False), NormalizeUnitSphere()])\n","\n","train_data = SegmentationFaust(\n","    root=root,\n","    pre_transform=pre_transform,\n",")\n","test_data = SegmentationFaust(\n","    root=root,\n","    train=False,\n","    pre_transform=pre_transform,\n",")\n","train_loader = DataLoader(train_data,  shuffle=True)\n","test_loader = DataLoader(test_data, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing...\n","Done!\n","/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zAxLPNA1vB2L"},"source":["#Training Model\n","Let's train the model on the training dataset.\n","\n","*Note: The model training pipeline is solely provided for demonstration purposes. It takes a few hundred epochs until the network outputs reasonable node-level class predictions. Thus, we suggest executing the training pipeline from the command line, and using a tensorboard to track the training.*"]},{"cell_type":"code","metadata":{"id":"oIMlUR_6Br0V"},"source":["lr = 0.001\n","num_epochs = 50\n","best_test_acc = 0.0\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","loss_fn = torch.nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nfXNyI_BJ9A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"29e5e652-7540-45af-8469-9e9472bb5c8a"},"source":["with tqdm(range(num_epochs), unit=\"Epoch\") as tepochs:\n","    for epoch in tepochs:\n","        train_loss = train(net, train_loader, optimizer, loss_fn, device)\n","        train_acc, test_acc = test(net, train_loader, test_loader, device)\n","\n","        tepochs.set_postfix(\n","            train_loss=train_loss,\n","            train_accuracy=100 * train_acc,\n","            test_accuracy=100 * test_acc,\n","        )\n","        sleep(0.1)\n","\n","        if test_acc > best_test_acc:\n","            best_test_acc = test_acc\n","            torch.save(net.state_dict(), \"/content/checkpoint_best_colab\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 49/50 [05:20<00:06,  6.52s/Epoch, test_accuracy=61.8, train_accuracy=63.2, train_loss=0.422]"]}]},{"cell_type":"markdown","metadata":{"id":"52dx3SDVOzoO"},"source":["#Model Evaluation & Prediction Visualization\n","\n","Now that we have trained the model, let's evaluate its performance on the test-dataset and visualize its predictions of body-part labels.\n","\n","Note that we also provide a pretrained model, `/content/checkpoint_best_pretrained`. By calling the function `get_best_model` the best model is returned."]},{"cell_type":"code","metadata":{"id":"vYDxnQ9_PFvk"},"source":["!gdown --id 1HxW_JsmFk3CBMvv3Pj3ztSmOOWKxTQtl # download pretrained model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7EPfgCAE8y7"},"source":["def load_model(model_params, path_to_checkpoint, device):\n","    try:\n","        model = MeshSeg(**model_params)\n","        model.load_state_dict(\n","            torch.load(str(path_to_checkpoint)),\n","            strict=True,\n","        )\n","        model.to(device)\n","        return model\n","    except RuntimeError as err_msg:\n","        raise ValueError(\n","            f\"Given checkpoint {str(path_to_checkpoint)} could\"\n","            f\" not be loaded. {err_msg}\"\n","        )\n","\n","def get_best_model(model_params, dataset, device):\n","    path_to_pretrained_model = Path(\"/content/checkpoint_best_pretrained\")\n","    path_to_trained_model = Path(\"/content/checkpoint_best_colab\")\n","\n","    pretrained_model = load_model(\n","        model_params,\n","        path_to_pretrained_model,\n","        device,\n","    )\n","    if not path_to_trained_model.exists():\n","        return pretrained_model\n","\n","    trained_model = load_model(\n","        model_params,\n","        path_to_trained_model,\n","        device,\n","    )\n","    acc_pretrained = evaluate_performance(dataset, pretrained_model, device)\n","    acc_trained = evaluate_performance(dataset, trained_model, device)\n","    if acc_pretrained > acc_trained:\n","        return pretrained_model\n","    return trained_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dW4T4p7K5PP"},"source":["net = get_best_model(model_params, test_loader, device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adC9ICDMPxT3"},"source":["##Define Segmentation Color Map\n","We define a mapping between the semantic body-part classes and RGB colors. This mapping is used to visualize the prediceted node labels."]},{"cell_type":"code","metadata":{"id":"lOJ6bIM8b1Ph"},"source":["segmentation_colors = dict(\n","    head=torch.tensor([255, 0, 0], dtype=torch.int),\n","    torso=torch.tensor([255, 0, 255], dtype=torch.int),\n","    left_arm=torch.tensor([255, 255, 0], dtype=torch.int),\n","    left_hand=torch.tensor([255, 128, 0], dtype=torch.int),\n","    right_arm=torch.tensor([0, 255, 0], dtype=torch.int),\n","    right_hand=torch.tensor([0, 255, 128], dtype=torch.int),\n","    left_upper_leg=torch.tensor([0, 128, 255], dtype=torch.int),\n","    left_lower_leg=torch.tensor([0, 255, 255], dtype=torch.int),\n","    left_foot=torch.tensor([0, 0, 255], dtype=torch.int),\n","    right_upper_leg=torch.tensor([128, 0, 255], dtype=torch.int),\n","    right_lower_leg=torch.tensor([128, 255, 0], dtype=torch.int),\n","    right_foot=torch.tensor([255, 0, 128], dtype=torch.int)\n",")\n","map_seg_id_to_color = dict(\n","    (_value, segmentation_colors[_key])\n","    for _key, _value in train_data.map_seg_label_to_id.items()\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgY0aCK-GADj"},"source":["@torch.no_grad()\n","def visualize_prediction(net, data, device, map_seg_id_to_color):\n","    \"\"\"Visualization of predicted segmentation mask.\"\"\"\n","    def _map_seg_label_to_color(seg_ids, map_seg_id_to_color):\n","        return torch.vstack(\n","            [map_seg_id_to_color[int(seg_ids[idx])] for idx in range(seg_ids.shape[0])]\n","        )\n","\n","    data = data.to(device)\n","    predictions = net(data)\n","    predicted_seg_labels = predictions.argmax(dim=-1, keepdim=True)\n","    mesh_colors = _map_seg_label_to_color(predicted_seg_labels, map_seg_id_to_color)\n","    segmented_mesh = trimesh.base.Trimesh(\n","        vertices=data.x.cpu().numpy(),\n","        faces=data.face.t().cpu().numpy(),\n","        process=False,\n","    )\n","    segmented_mesh.visual.vertex_colors = mesh_colors.cpu().numpy()\n","    return segmented_mesh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wctE1kOTtR6I"},"source":["segmented_meshes = []\n","mesh_ids = [0, 1, 10, 11]\n","for idx, mesh_id in enumerate(mesh_ids):\n","    segmented_mesh = visualize_prediction(net, test_data[mesh_id], device, map_seg_id_to_color)\n","    segmented_mesh.vertices += [idx * 1.0, 0.0, 0.0]\n","    segmented_meshes.append(segmented_mesh)\n","\n","scene = trimesh.scene.Scene(segmented_meshes)\n","scene.show()"],"execution_count":null,"outputs":[]}]}